{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Decision Tree**"
      ],
      "metadata": {
        "id": "n0JYJnq8oY8n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### **1. What is a Decision Tree, and How Does It Work?**\n",
        "\n",
        "A **Decision Tree** is a **supervised learning algorithm** used for both **classification** and **regression** tasks.\n",
        "\n",
        "#### üîπ How it works:\n",
        "\n",
        "* The data is split into subsets based on the **feature that results in the best split** (lowest impurity).\n",
        "* This splitting continues **recursively** forming a tree-like structure:\n",
        "\n",
        "  * **Root Node** ‚Üí first decision\n",
        "  * **Internal Nodes** ‚Üí further decisions\n",
        "  * **Leaf Nodes** ‚Üí final predictions (class or value)\n",
        "\n",
        "#### Example:\n",
        "\n",
        "```text\n",
        "Is age > 30?\n",
        "‚îú‚îÄ‚îÄ Yes ‚Üí Is income > 50K?\n",
        "‚îÇ   ‚îú‚îÄ‚îÄ Yes ‚Üí Approve loan\n",
        "‚îÇ   ‚îî‚îÄ‚îÄ No ‚Üí Reject loan\n",
        "‚îî‚îÄ‚îÄ No ‚Üí Reject loan\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **2. What are Impurity Measures in Decision Trees?**\n",
        "\n",
        "Impurity measures tell us **how mixed** the classes are in a node. A pure node has samples from **only one class**.\n",
        "\n",
        "Common impurity measures:\n",
        "\n",
        "* **Gini Impurity** (used in CART algorithm)\n",
        "* **Entropy** (used in ID3, C4.5 algorithms)\n",
        "* (For regression: variance or mean squared error)\n",
        "\n",
        "The **goal** is to **reduce impurity** at each split.\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Mathematical Formula for Gini Impurity**\n",
        "\n",
        "Gini impurity measures the probability of **incorrect classification** if a label is randomly chosen.\n",
        "\n",
        "$$\n",
        "\\text{Gini}(D) = 1 - \\sum_{i=1}^{C} p_i^2\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $D$ is the dataset at a node\n",
        "* $C$ is the number of classes\n",
        "* $p_i$ is the proportion of examples belonging to class $i$\n",
        "\n",
        "‚úÖ Gini is **minimum (0)** when all samples belong to one class.\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Mathematical Formula for Entropy**\n",
        "\n",
        "Entropy measures the **amount of uncertainty** or **disorder** in the data.\n",
        "\n",
        "$$\n",
        "\\text{Entropy}(D) = - \\sum_{i=1}^{C} p_i \\log_2(p_i)\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $p_i$ is the proportion of class $i$ in dataset $D$\n",
        "\n",
        "‚úÖ Entropy is **0** when all samples belong to one class (pure node), and **maximum** when classes are equally distributed.\n",
        "\n",
        "---\n",
        "\n",
        "### **5. What is Information Gain, and How Is It Used in Decision Trees?**\n",
        "\n",
        "**Information Gain (IG)** measures the **reduction in entropy** after a dataset is split on a feature.\n",
        "\n",
        "#### üîπ Formula:\n",
        "\n",
        "$$\n",
        "\\text{Information Gain} = \\text{Entropy}(parent) - \\sum_{k=1}^{K} \\frac{|D_k|}{|D|} \\cdot \\text{Entropy}(D_k)\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $D$: original dataset\n",
        "* $D_k$: subsets after split\n",
        "* $|D_k|$: number of samples in each subset\n",
        "* $K$: number of subsets\n",
        "\n",
        "#### üî∏ Use in Decision Trees:\n",
        "\n",
        "* For each feature, calculate **information gain**\n",
        "* Choose the feature with the **highest information gain**\n",
        "* Repeat recursively to grow the tree\n",
        "\n",
        "---\n",
        "\n",
        "### üìå Summary Table\n",
        "\n",
        "| Concept               | Formula / Description                                          |\n",
        "| --------------------- | -------------------------------------------------------------- |\n",
        "| **Decision Tree**     | A tree model that splits data using features to make decisions |\n",
        "| **Gini Impurity**     | $1 - \\sum p_i^2$                                               |\n",
        "| **Entropy**           | $-\\sum p_i \\log_2(p_i)$                                        |\n",
        "| **Information Gain**  | Entropy(parent) ‚àí Weighted sum of Entropy(children)            |\n",
        "| **Impurity Measures** | Help decide which feature to split on                          |\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### **6. Difference Between Gini Impurity and Entropy**\n",
        "\n",
        "| Feature        | **Gini Impurity**                | **Entropy**                     |\n",
        "| -------------- | -------------------------------- | ------------------------------- |\n",
        "| Formula        | $1 - \\sum p_i^2$                 | $-\\sum p_i \\log_2(p_i)$         |\n",
        "| Range          | \\[0, 0.5] (for binary)           | \\[0, 1]                         |\n",
        "| Interpretation | Probability of misclassification | Measure of disorder/uncertainty |\n",
        "| Computation    | Faster                           | Slightly slower (uses log)      |\n",
        "| Used In        | CART (default in `sklearn`)      | ID3, C4.5                       |\n",
        "\n",
        "‚úÖ Both give similar results; **Gini is preferred for performance**, while **Entropy is more theoretical**.\n",
        "\n",
        "---\n",
        "\n",
        "### **7. Mathematical Explanation Behind Decision Trees**\n",
        "\n",
        "At each node:\n",
        "\n",
        "* The algorithm evaluates **all features** and **all possible split points**\n",
        "* It chooses the split that gives the **highest reduction in impurity**\n",
        "* Repeats the process **recursively** until stopping criteria is met\n",
        "\n",
        "#### Decision Function:\n",
        "\n",
        "* Let $X$ be the feature space\n",
        "* Let $A \\subseteq X$ be a subset defined by some split\n",
        "* The algorithm aims to **partition data** so that each group is more homogeneous (pure)\n",
        "\n",
        "At each split:\n",
        "\n",
        "$$\n",
        "\\text{Gain} = \\text{Impurity}_{\\text{parent}} - \\sum \\left( \\frac{n_k}{n} \\cdot \\text{Impurity}_{k} \\right)\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### **8. What is Pre-Pruning in Decision Trees?**\n",
        "\n",
        "**Pre-pruning** stops the tree **early** while it's being built to prevent overfitting.\n",
        "\n",
        "#### Examples:\n",
        "\n",
        "* Stop if maximum depth is reached (`max_depth`)\n",
        "* Stop if node has fewer than `min_samples_split`\n",
        "* Stop if impurity improvement is below `min_impurity_decrease`\n",
        "\n",
        "‚úÖ It's a **preventive** strategy.\n",
        "\n",
        "---\n",
        "\n",
        "### **9. What is Post-Pruning in Decision Trees?**\n",
        "\n",
        "**Post-pruning** builds the full tree first, then **removes branches** that have little predictive power.\n",
        "\n",
        "#### Steps:\n",
        "\n",
        "1. Grow the full tree.\n",
        "2. Evaluate performance on **validation set**.\n",
        "3. **Prune** (cut) nodes if it improves generalization.\n",
        "\n",
        "‚úÖ More accurate than pre-pruning, but **computationally expensive**.\n",
        "\n",
        "---\n",
        "\n",
        "### **10. Difference Between Pre-Pruning and Post-Pruning**\n",
        "\n",
        "| Feature      | **Pre-Pruning**      | **Post-Pruning**          |\n",
        "| ------------ | -------------------- | ------------------------- |\n",
        "| When Applied | During tree building | After tree is fully grown |\n",
        "| Speed        | Faster               | Slower                    |\n",
        "| Risk         | Might underfit       | Better generalization     |\n",
        "| Strategy     | Preventive           | Corrective                |\n",
        "\n",
        "---\n",
        "\n",
        "### **11. What is a Decision Tree Regressor?**\n",
        "\n",
        "A **Decision Tree Regressor** predicts **continuous (real-valued)** outputs instead of categories.\n",
        "\n",
        "#### How it works:\n",
        "\n",
        "* Splits the data based on feature thresholds\n",
        "* Each leaf node predicts the **mean value** of samples in that node\n",
        "* Uses **MSE (Mean Squared Error)** or **MAE** as impurity measure\n",
        "\n",
        "‚úÖ Example use: Predicting house prices, stock values, etc.\n",
        "\n",
        "---\n",
        "\n",
        "### **12. Advantages and Disadvantages of Decision Trees**\n",
        "\n",
        "#### ‚úÖ Advantages:\n",
        "\n",
        "* Easy to understand and visualize\n",
        "* Requires little data preprocessing\n",
        "* Handles both numerical and categorical data\n",
        "* Non-linear relationships are captured\n",
        "\n",
        "#### ‚ùå Disadvantages:\n",
        "\n",
        "* Easily overfits (especially deep trees)\n",
        "* Unstable to small data changes\n",
        "* Greedy splitting may miss global optimum\n",
        "* Biased toward features with more levels (without correction)\n",
        "\n",
        "---\n",
        "\n",
        "### **13. How Does a Decision Tree Handle Missing Values?**\n",
        "\n",
        "Depending on implementation:\n",
        "\n",
        "#### üß† Common strategies:\n",
        "\n",
        "* **Imputation**: fill missing values before training (e.g., mean, median)\n",
        "* **Surrogate splits**: use alternative features when primary split feature is missing (in some libraries like R)\n",
        "* **Skipping instances** during split evaluation (less common)\n",
        "\n",
        "In `sklearn`, you must **impute** missing values manually (using `SimpleImputer`).\n",
        "\n",
        "---\n",
        "\n",
        "### **14. How Does a Decision Tree Handle Categorical Features?**\n",
        "\n",
        "In `scikit-learn`, **categorical features must be encoded**, such as:\n",
        "\n",
        "* **Label encoding** (if ordinal)\n",
        "* **One-hot encoding** (if nominal)\n",
        "\n",
        "Some other libraries (e.g., **LightGBM**, **CatBoost**) support categorical features **natively**.\n",
        "\n",
        "---\n",
        "\n",
        "### **15. Real-World Applications of Decision Trees**\n",
        "\n",
        "| Domain            | Use Case                                |\n",
        "| ----------------- | --------------------------------------- |\n",
        "| **Healthcare**    | Disease diagnosis based on symptoms     |\n",
        "| **Finance**       | Credit scoring, loan approval           |\n",
        "| **Marketing**     | Customer segmentation, churn prediction |\n",
        "| **E-commerce**    | Product recommendation                  |\n",
        "| **Manufacturing** | Quality control decision rules          |\n",
        "| **HR**            | Predicting employee attrition           |\n",
        "\n",
        "---\n",
        "\n",
        "### üìå Quick Summary:\n",
        "\n",
        "| #  | Concept              | Key Idea                                       |\n",
        "| -- | -------------------- | ---------------------------------------------- |\n",
        "| 6  | Gini vs Entropy      | Gini is faster, Entropy is more info-theoretic |\n",
        "| 7  | Math Behind Tree     | Greedy impurity-based splitting                |\n",
        "| 8  | Pre-Pruning          | Stop growing early                             |\n",
        "| 9  | Post-Pruning         | Cut back after full tree                       |\n",
        "| 10 | Difference           | Pre = fast, Post = accurate                    |\n",
        "| 11 | Tree Regressor       | Predicts continuous values                     |\n",
        "| 12 | Pros/Cons            | Interpretable but prone to overfitting         |\n",
        "| 13 | Missing Values       | Imputation or surrogate splits                 |\n",
        "| 14 | Categorical Features | Encode or use libraries that support them      |\n",
        "| 15 | Applications         | Health, finance, HR, etc.                      |\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "b1eMTM-koePv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gwBqhwKfnBt5"
      },
      "outputs": [],
      "source": []
    }
  ]
}